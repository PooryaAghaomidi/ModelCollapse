{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ljWE-qDpqEB4"
   },
   "source": [
    "# Preparation"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "seed_value= 42\n",
    "\n",
    "import random\n",
    "random.seed(seed_value)\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(seed_value)\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.config.run_functions_eagerly(True)\n",
    "tf.random.set_seed(seed_value)\n",
    "tf.keras.utils.set_random_seed(seed_value)"
   ],
   "metadata": {
    "id": "sUr3ZyJyqgvf",
    "ExecuteTime": {
     "end_time": "2025-02-17T20:01:30.691834Z",
     "start_time": "2025-02-17T20:01:25.243728Z"
    }
   },
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "print(tf.__version__)\n",
    "print(tf.config.list_physical_devices('GPU'))"
   ],
   "metadata": {
    "id": "lFlwHbbEqm_C",
    "outputId": "425f2df4-2ddd-4dfc-ad66-676b769608aa",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "ExecuteTime": {
     "end_time": "2025-02-17T20:01:31.340765Z",
     "start_time": "2025-02-17T20:01:30.691834Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.9.0\n",
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "UxBHr1ztqECA",
    "ExecuteTime": {
     "end_time": "2025-02-17T20:01:31.388040Z",
     "start_time": "2025-02-17T20:01:31.340765Z"
    }
   },
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from gudhi.tensorflow import CubicalLayer"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "pQGJI2aIqECB",
    "ExecuteTime": {
     "end_time": "2025-02-17T20:01:31.403683Z",
     "start_time": "2025-02-17T20:01:31.388040Z"
    }
   },
   "source": [
    "num_epochs  = 20\n",
    "batch_size  = 32\n",
    "num_classes = 10\n",
    "shape       = (28, 28, 1)\n",
    "lr          = 0.0003\n",
    "opt         = keras.optimizers.Adam(learning_rate=lr)\n",
    "los         = keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "latent_dim  = 28"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VkbDK0AAqECC"
   },
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "IyJalcA1qECC",
    "outputId": "4e3fe176-81ee-48f4-9260-7456854ed5a7",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "ExecuteTime": {
     "end_time": "2025-02-17T20:01:33.309149Z",
     "start_time": "2025-02-17T20:01:31.403683Z"
    }
   },
   "source": [
    "def prepare_data(main_path):\n",
    "    with np.load(main_path) as data:\n",
    "        x_train, y_train = data['x_train'], data['y_train']\n",
    "    \n",
    "    x_train = x_train.astype(\"float32\") / 255.0\n",
    "    x_train = np.reshape(x_train, (-1, 28, 28, 1))\n",
    "    y_train = keras.utils.to_categorical(y_train, 10)\n",
    "    train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "    train_dataset = train_dataset.shuffle(buffer_size=1024).batch(batch_size)\n",
    "    \n",
    "    return train_dataset\n",
    "\n",
    "generator_in_channels = latent_dim + num_classes\n",
    "discriminator_in_channels = shape[2] + num_classes\n",
    "\n",
    "train_dataset = prepare_data(main_path=\"../Dataset/mnist.npz\")"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xh4A2BtfqECF",
    "ExecuteTime": {
     "end_time": "2024-11-22T23:06:19.033004900Z",
     "start_time": "2024-11-22T23:06:18.970186300Z"
    }
   },
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "-E8lf3QbqECF",
    "ExecuteTime": {
     "end_time": "2025-02-17T20:01:33.340401Z",
     "start_time": "2025-02-17T20:01:33.309149Z"
    }
   },
   "source": [
    "def get_models():\n",
    "    discriminator = keras.Sequential([keras.layers.InputLayer((shape[0], shape[1], discriminator_in_channels)),\n",
    "                                      layers.Conv2D(64, (3, 3), strides=(2, 2), padding=\"same\"),\n",
    "                                      layers.LeakyReLU(alpha=0.2),\n",
    "                                      layers.Conv2D(128, (3, 3), strides=(2, 2), padding=\"same\"),\n",
    "                                      layers.LeakyReLU(alpha=0.2),\n",
    "                                      layers.GlobalMaxPooling2D(),\n",
    "                                      layers.Dense(1)],\n",
    "                                     name=\"discriminator\")\n",
    "    \n",
    "    generator = keras.Sequential([keras.layers.InputLayer((generator_in_channels,)),\n",
    "                                  layers.Dense(7 * 7 * generator_in_channels),\n",
    "                                  layers.LeakyReLU(alpha=0.2),\n",
    "                                  layers.Reshape((7, 7, generator_in_channels)),\n",
    "                                  layers.Conv2DTranspose(128, (4, 4), strides=(2, 2), padding=\"same\"),\n",
    "                                  layers.LeakyReLU(alpha=0.2),\n",
    "                                  layers.Conv2DTranspose(128, (4, 4), strides=(2, 2), padding=\"same\"),\n",
    "                                  layers.LeakyReLU(alpha=0.2),\n",
    "                                  layers.Conv2D(1, (7, 7), padding=\"same\", activation=\"sigmoid\")],\n",
    "                                 name=\"generator\")\n",
    "    \n",
    "    return discriminator, generator"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "nXR9LbG3qECG",
    "ExecuteTime": {
     "end_time": "2025-02-17T20:01:33.403305Z",
     "start_time": "2025-02-17T20:01:33.340401Z"
    }
   },
   "source": [
    "class ConditionalGAN(keras.Model):\n",
    "    def __init__(self, discriminator, generator, latent_dim, accumulation=False):\n",
    "        super().__init__()\n",
    "        self.discriminator = discriminator\n",
    "        self.generator = generator\n",
    "        self.latent_dim = latent_dim\n",
    "        self.accumulation = accumulation\n",
    "        self.seed_generator = tf.random.Generator.from_seed(seed_value)\n",
    "        self.gen_loss_tracker = keras.metrics.Mean(name=\"generator_loss\")\n",
    "        self.disc_loss_tracker = keras.metrics.Mean(name=\"discriminator_loss\")\n",
    "        self.cubical_layer = CubicalLayer(homology_dimensions=[0, 1, 2])\n",
    "        \n",
    "        self.digits = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
    "        \n",
    "        self.ph_real = {str(i): tf.zeros((0, 2), dtype=tf.float32) for i in range(10)}\n",
    "        self.ph_fake = {str(i): tf.zeros((0, 2), dtype=tf.float32) for i in range(10)}\n",
    "\n",
    "\n",
    "    def SWD(self, PD1, PD2, num_projections=50):\n",
    "        angles = tf.random.uniform([num_projections, 2], minval=-1, maxval=1)\n",
    "        angles /= tf.norm(angles, axis=-1, keepdims=True)\n",
    "        \n",
    "        proj1 = tf.linalg.matmul(PD1, tf.transpose(angles))\n",
    "        proj2 = tf.linalg.matmul(PD2, tf.transpose(angles))\n",
    "        \n",
    "        proj1 = tf.sort(proj1, axis=0)\n",
    "        proj2 = tf.sort(proj2, axis=0)\n",
    "        \n",
    "        target_size = tf.maximum(tf.shape(proj1)[0], tf.shape(proj2)[0])\n",
    "        proj1 = tf.image.resize(proj1[None, :, :], [target_size, num_projections])[0]\n",
    "        proj2 = tf.image.resize(proj2[None, :, :], [target_size, num_projections])[0]\n",
    "    \n",
    "        return tf.reduce_mean(tf.abs(proj1 - proj2))\n",
    "    \n",
    "    \n",
    "    def process_digit(self, indices, X_real, X_fake):\n",
    "        real_subset = tf.gather(X_real, indices)\n",
    "        fake_subset = tf.gather(X_fake, indices)\n",
    "    \n",
    "        real_dgms = self.cubical_layer.call(real_subset)\n",
    "        fake_dgms = self.cubical_layer.call(fake_subset)\n",
    "        \n",
    "        real_concat = tf.concat([real_dgms[0][0], real_dgms[1][0], real_dgms[2][0]], axis=0)\n",
    "        fake_concat = tf.concat([fake_dgms[0][0], fake_dgms[1][0], fake_dgms[2][0]], axis=0)\n",
    "    \n",
    "        return real_concat, fake_concat\n",
    "    \n",
    "    \n",
    "    def skip_digit(self):\n",
    "        return tf.zeros((0, 2), dtype=tf.float32), tf.zeros((0, 2), dtype=tf.float32)\n",
    "\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [self.gen_loss_tracker, self.disc_loss_tracker]\n",
    "\n",
    "\n",
    "    def compile(self, d_optimizer, g_optimizer, loss_fn):\n",
    "        super().compile()\n",
    "        self.d_optimizer = d_optimizer\n",
    "        self.g_optimizer = g_optimizer\n",
    "        self.loss_fn = loss_fn\n",
    "\n",
    "\n",
    "    def train_step(self, data):\n",
    "        real_images, one_hot_labels = data\n",
    "        \n",
    "        # Reshape and repeat one-hot labels to match the image dimensions\n",
    "        image_one_hot_labels = one_hot_labels[:, :, None, None]\n",
    "        image_one_hot_labels = tf.repeat(image_one_hot_labels, repeats=[real_images.shape[1] * real_images.shape[2]], axis=-1)\n",
    "        image_one_hot_labels = tf.reshape(image_one_hot_labels, (-1, real_images.shape[1], real_images.shape[2], one_hot_labels.shape[-1]))\n",
    "    \n",
    "        # Batch size\n",
    "        batch_size = tf.shape(real_images)[0]\n",
    "        \n",
    "        # Generate random latent vectors\n",
    "        random_latent_vectors = self.seed_generator.normal(shape=(batch_size, self.latent_dim), dtype=tf.float32)\n",
    "        random_vector_labels = tf.concat([random_latent_vectors, one_hot_labels], axis=1)\n",
    "    \n",
    "        # Generate fake images\n",
    "        generated_images = self.generator(random_vector_labels)\n",
    "    \n",
    "        # Combine fake and real images with their labels\n",
    "        fake_image_and_labels = tf.concat([generated_images, image_one_hot_labels], axis=-1)\n",
    "        real_image_and_labels = tf.concat([real_images, image_one_hot_labels], axis=-1)\n",
    "        combined_images = tf.concat([fake_image_and_labels, real_image_and_labels], axis=0)\n",
    "    \n",
    "        # Labels for discriminator\n",
    "        labels = tf.concat([tf.ones((batch_size, 1)), tf.zeros((batch_size, 1))], axis=0)\n",
    "    \n",
    "        # Train discriminator\n",
    "        with tf.GradientTape() as tape:\n",
    "            predictions = self.discriminator(combined_images)\n",
    "            d_loss = self.loss_fn(labels, predictions)\n",
    "        grads = tape.gradient(d_loss, self.discriminator.trainable_weights)\n",
    "        self.d_optimizer.apply_gradients(zip(grads, self.discriminator.trainable_weights))\n",
    "    \n",
    "        # Train generator\n",
    "        misleading_labels = tf.zeros((batch_size, 1))\n",
    "    \n",
    "        with tf.GradientTape() as tape:\n",
    "            fake_images = self.generator(random_vector_labels)\n",
    "            fake_image_and_labels = tf.concat([fake_images, image_one_hot_labels], axis=-1)\n",
    "            predictions = self.discriminator(fake_image_and_labels)\n",
    "            g_loss = self.loss_fn(misleading_labels, predictions)\n",
    "        \n",
    "            # Calculate Persistent Homology\n",
    "            y_train = tf.argmax(one_hot_labels, axis=1)\n",
    "            X_real = (real_images - tf.reduce_min(real_images)) / (tf.reduce_max(real_images) - tf.reduce_min(real_images))\n",
    "            X_fake = (fake_images - tf.reduce_min(fake_images)) / (tf.reduce_max(fake_images) - tf.reduce_min(fake_images))\n",
    "            \n",
    "            ph_losses = 0\n",
    "            valid_digits = 0\n",
    "            for digit in self.digits:\n",
    "                indices = tf.where(y_train == digit)[:, 0]\n",
    "                \n",
    "                if tf.size(indices) == 0:\n",
    "                    continue\n",
    "                else:\n",
    "                    real_concat, fake_concat = self.process_digit(indices, X_real, X_fake)\n",
    "                \n",
    "                if self.accumulation:\n",
    "                    self.ph_real[str(digit)] = tf.concat([self.ph_real[str(digit)], real_concat], axis=0)\n",
    "                    self.ph_fake[str(digit)] = tf.concat([self.ph_fake[str(digit)], fake_concat], axis=0)\n",
    "                    ph_losses += self.SWD(self.ph_real[str(digit)], self.ph_fake[str(digit)])\n",
    "                else:\n",
    "                    ph_losses += self.SWD(real_concat, fake_concat)\n",
    "                    \n",
    "                valid_digits += 1\n",
    "                \n",
    "            ph_losses = ph_losses / valid_digits\n",
    "            \n",
    "            g_total_loss = g_loss + ph_losses\n",
    "            \n",
    "        grads = tape.gradient(g_total_loss, self.generator.trainable_weights)\n",
    "        self.g_optimizer.apply_gradients(zip(grads, self.generator.trainable_weights))\n",
    "    \n",
    "        # Update loss trackers\n",
    "        self.gen_loss_tracker.update_state(g_total_loss)\n",
    "        self.disc_loss_tracker.update_state(d_loss)\n",
    "        return {\n",
    "            \"loss\": self.gen_loss_tracker.result() + self.disc_loss_tracker.result(),\n",
    "            \"g_loss\": self.gen_loss_tracker.result(),\n",
    "            \"d_loss\": self.disc_loss_tracker.result(),\n",
    "        }"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-17T20:01:33.693343Z",
     "start_time": "2025-02-17T20:01:33.403305Z"
    }
   },
   "cell_type": "code",
   "source": [
    "discriminator, generator = get_models()\n",
    "cond_gan = ConditionalGAN(discriminator=discriminator, generator=generator, latent_dim=latent_dim)\n",
    "cond_gan.compile(d_optimizer=opt, g_optimizer=opt, loss_fn=los)"
   ],
   "outputs": [],
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-SJbTfPtqECG",
    "ExecuteTime": {
     "end_time": "2024-11-22T23:10:10.564855800Z",
     "start_time": "2024-11-22T23:06:19.064800900Z"
    }
   },
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "source": "cond_gan.fit(train_dataset, epochs=num_epochs, batch_size=batch_size, verbose=1)",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-02-17T20:01:33.693343Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "   5/1875 [..............................] - ETA: 1:27:38 - loss: 1.4477 - g_loss: 0.7674 - d_loss: 0.6803"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
