{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ljWE-qDpqEB4"
   },
   "source": [
    "# Preparation"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "seed_value= 42\n",
    "\n",
    "import random\n",
    "random.seed(seed_value)\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(seed_value)\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(seed_value)\n",
    "tf.keras.utils.set_random_seed(seed_value)"
   ],
   "metadata": {
    "id": "sUr3ZyJyqgvf",
    "ExecuteTime": {
     "end_time": "2025-02-23T22:54:21.153760Z",
     "start_time": "2025-02-23T22:54:16.099476Z"
    }
   },
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "print(tf.__version__)\n",
    "print(tf.config.list_physical_devices('GPU'))"
   ],
   "metadata": {
    "id": "lFlwHbbEqm_C",
    "outputId": "425f2df4-2ddd-4dfc-ad66-676b769608aa",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "ExecuteTime": {
     "end_time": "2025-02-23T22:54:21.778482Z",
     "start_time": "2025-02-23T22:54:21.153760Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.9.0\n",
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "UxBHr1ztqECA",
    "ExecuteTime": {
     "end_time": "2025-02-23T22:54:21.825368Z",
     "start_time": "2025-02-23T22:54:21.778482Z"
    }
   },
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from gudhi.tensorflow import CubicalLayer"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "pQGJI2aIqECB",
    "ExecuteTime": {
     "end_time": "2025-02-23T22:54:21.840993Z",
     "start_time": "2025-02-23T22:54:21.825368Z"
    }
   },
   "source": [
    "num_epochs  = 20\n",
    "batch_size  = 32\n",
    "num_classes = 10\n",
    "shape       = (28, 28, 1)\n",
    "lr          = 0.0003\n",
    "opt         = keras.optimizers.Adam(learning_rate=lr)\n",
    "los         = keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "latent_dim  = 28"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VkbDK0AAqECC"
   },
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "IyJalcA1qECC",
    "outputId": "4e3fe176-81ee-48f4-9260-7456854ed5a7",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "ExecuteTime": {
     "end_time": "2025-02-23T22:54:23.258257Z",
     "start_time": "2025-02-23T22:54:21.840993Z"
    }
   },
   "source": [
    "def prepare_data(main_path):\n",
    "    with np.load(main_path) as data:\n",
    "        x_train, y_train = data['x_train'], data['y_train']\n",
    "    \n",
    "    x_train = x_train.astype(\"float32\") / 255.0\n",
    "    x_train = np.reshape(x_train, (-1, 28, 28, 1))\n",
    "    y_train = keras.utils.to_categorical(y_train, 10)\n",
    "    train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "    train_dataset = train_dataset.shuffle(buffer_size=1024).batch(batch_size)\n",
    "    \n",
    "    return train_dataset\n",
    "\n",
    "generator_in_channels = latent_dim + num_classes\n",
    "discriminator_in_channels = shape[2] + num_classes\n",
    "\n",
    "train_dataset = prepare_data(main_path=\"../Dataset/mnist.npz\")"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xh4A2BtfqECF",
    "ExecuteTime": {
     "end_time": "2024-11-22T23:06:19.033004900Z",
     "start_time": "2024-11-22T23:06:18.970186300Z"
    }
   },
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "-E8lf3QbqECF",
    "ExecuteTime": {
     "end_time": "2025-02-23T22:54:23.274282Z",
     "start_time": "2025-02-23T22:54:23.258257Z"
    }
   },
   "source": [
    "def get_models():\n",
    "    discriminator = keras.Sequential([keras.layers.InputLayer((shape[0], shape[1], discriminator_in_channels)),\n",
    "                                      layers.Conv2D(64, (3, 3), strides=(2, 2), padding=\"same\"),\n",
    "                                      layers.LeakyReLU(alpha=0.2),\n",
    "                                      layers.Conv2D(128, (3, 3), strides=(2, 2), padding=\"same\"),\n",
    "                                      layers.LeakyReLU(alpha=0.2),\n",
    "                                      layers.GlobalMaxPooling2D(),\n",
    "                                      layers.Dense(1)],\n",
    "                                     name=\"discriminator\")\n",
    "    \n",
    "    generator = keras.Sequential([keras.layers.InputLayer((generator_in_channels,)),\n",
    "                                  layers.Dense(7 * 7 * generator_in_channels),\n",
    "                                  layers.LeakyReLU(alpha=0.2),\n",
    "                                  layers.Reshape((7, 7, generator_in_channels)),\n",
    "                                  layers.Conv2DTranspose(128, (4, 4), strides=(2, 2), padding=\"same\"),\n",
    "                                  layers.LeakyReLU(alpha=0.2),\n",
    "                                  layers.Conv2DTranspose(128, (4, 4), strides=(2, 2), padding=\"same\"),\n",
    "                                  layers.LeakyReLU(alpha=0.2),\n",
    "                                  layers.Conv2D(1, (7, 7), padding=\"same\", activation=\"sigmoid\")],\n",
    "                                 name=\"generator\")\n",
    "    \n",
    "    return discriminator, generator"
   ],
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-23T22:54:53.524429Z",
     "start_time": "2025-02-23T22:54:53.487252Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class PersistenceSetLoss(tf.keras.losses.Loss):\n",
    "    def __init__(self, gamma=1.0, mu=0.1, max_value=2000000, scale=1e-1, **kwargs):\n",
    "        \"\"\"\n",
    "        Custom loss function for comparing persistence diagrams of different sizes.\n",
    "        \n",
    "        Parameters:\n",
    "        - gamma: Controls how much long-lived features are weighted.\n",
    "        - mu: Controls how much we penalize differences in diagram sizes.\n",
    "        - max_value: Maximum value for the final loss function.\n",
    "        - scale: The scale that multiplies on the final loss value.\n",
    "        \"\"\"\n",
    "        super().__init__(**kwargs)\n",
    "        self.gamma = gamma\n",
    "        self.mu = mu\n",
    "        self.max_value = max_value\n",
    "        self.scale = scale\n",
    "        \n",
    "        self.cubical_layer = CubicalLayer(homology_dimensions=[0, 1, 2])\n",
    "        \n",
    "    def call_cubical_layer(self, X):\n",
    "        output = self.cubical_layer.call(X)\n",
    "        \n",
    "        flattened_output = []\n",
    "        for tuple_ in output:\n",
    "            flattened_output.extend(tuple_[0])\n",
    "        \n",
    "        return flattened_output\n",
    "        \n",
    "    @tf.custom_gradient\n",
    "    def call(self, real_images, fake_images):\n",
    "        \"\"\"\n",
    "        Computes the loss between two persistence diagrams P and Q.\n",
    "        \n",
    "        P: Tensor -> Persistence diagram with m points (birth, death)\n",
    "        Q: Tensor -> Persistence diagram with n points (birth, death)\n",
    "        \"\"\"\n",
    "        X_real = (real_images - tf.reduce_min(real_images)) / (tf.reduce_max(real_images) - tf.reduce_min(real_images))\n",
    "        X_fake = (fake_images - tf.reduce_min(fake_images)) / (tf.reduce_max(fake_images) - tf.reduce_min(fake_images))\n",
    "        \n",
    "        real_dgms = tf.py_function(self.call_cubical_layer, [X_real], Tout=[tf.float32] * 6)\n",
    "        fake_dgms = tf.py_function(self.call_cubical_layer, [X_fake], Tout=[tf.float32] * 6)\n",
    "        \n",
    "        real_dgms = tf.concat(real_dgms, axis=0)\n",
    "        P = tf.reshape(real_dgms, (-1, 2))\n",
    "        \n",
    "        fake_dgms = tf.concat(fake_dgms, axis=0)\n",
    "        Q = tf.reshape(fake_dgms, (-1, 2))\n",
    "        \n",
    "        P_persistence = P[:, 1] - P[:, 0]\n",
    "        Q_persistence = Q[:, 1] - Q[:, 0]\n",
    "\n",
    "        def feature_transform(persistence):\n",
    "            exp_term = tf.exp(-self.gamma * persistence)\n",
    "            return tf.stack([exp_term, persistence * exp_term], axis=-1)\n",
    "\n",
    "        P_features = feature_transform(P_persistence)\n",
    "        Q_features = feature_transform(Q_persistence)\n",
    "\n",
    "        P_sum = tf.reduce_sum(P_features, axis=0)\n",
    "        Q_sum = tf.reduce_sum(Q_features, axis=0)\n",
    "\n",
    "        feature_distance = tf.reduce_sum(tf.square(P_sum - Q_sum))\n",
    "\n",
    "        size_difference = tf.cast(tf.shape(P)[0] - tf.shape(Q)[0], tf.float32)\n",
    "        size_penalty = self.mu * tf.square(size_difference)        \n",
    "\n",
    "        ph_loss = tf.clip_by_value(feature_distance + size_penalty, 0.0, self.max_value) * self.scale\n",
    "        \n",
    "        def grad(dy):\n",
    "            grad_fake_images = tf.gradients(ph_loss, fake_images, grad_ys=dy)[0]\n",
    "            return None, grad_fake_images\n",
    "    \n",
    "        return ph_loss, grad"
   ],
   "outputs": [],
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "nXR9LbG3qECG",
    "ExecuteTime": {
     "end_time": "2025-02-23T22:54:53.940811Z",
     "start_time": "2025-02-23T22:54:53.893925Z"
    }
   },
   "source": [
    "class ConditionalGAN(keras.Model):\n",
    "    def __init__(self, discriminator, generator, latent_dim):\n",
    "        super().__init__()\n",
    "        self.discriminator = discriminator\n",
    "        self.generator = generator\n",
    "        self.latent_dim = latent_dim\n",
    "        self.seed_generator = tf.random.Generator.from_seed(seed_value)\n",
    "        self.gen_loss_tracker = keras.metrics.Mean(name=\"generator_loss\")\n",
    "        self.disc_loss_tracker = keras.metrics.Mean(name=\"discriminator_loss\")   \n",
    "        self.pd_loss = PersistenceSetLoss(scale=100)\n",
    "\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [self.gen_loss_tracker, self.disc_loss_tracker]\n",
    "\n",
    "\n",
    "    def compile(self, d_optimizer, g_optimizer, loss_fn):\n",
    "        super().compile()\n",
    "        self.d_optimizer = d_optimizer\n",
    "        self.g_optimizer = g_optimizer\n",
    "        self.loss_fn = loss_fn\n",
    "\n",
    "\n",
    "    def train_step(self, data):\n",
    "        real_images, one_hot_labels = data\n",
    "        \n",
    "        # Reshape and repeat one-hot labels to match the image dimensions\n",
    "        image_one_hot_labels = one_hot_labels[:, :, None, None]\n",
    "        image_one_hot_labels = tf.repeat(image_one_hot_labels, repeats=[real_images.shape[1] * real_images.shape[2]], axis=-1)\n",
    "        image_one_hot_labels = tf.reshape(image_one_hot_labels, (-1, real_images.shape[1], real_images.shape[2], one_hot_labels.shape[-1]))\n",
    "    \n",
    "        # Batch size\n",
    "        batch_size = tf.shape(real_images)[0]\n",
    "        \n",
    "        # Generate random latent vectors\n",
    "        random_latent_vectors = self.seed_generator.normal(shape=(batch_size, self.latent_dim), dtype=tf.float32)\n",
    "        random_vector_labels = tf.concat([random_latent_vectors, one_hot_labels], axis=1)\n",
    "    \n",
    "        # Generate fake images\n",
    "        generated_images = self.generator(random_vector_labels)\n",
    "    \n",
    "        # Combine fake and real images with their labels\n",
    "        fake_image_and_labels = tf.concat([generated_images, image_one_hot_labels], axis=-1)\n",
    "        real_image_and_labels = tf.concat([real_images, image_one_hot_labels], axis=-1)\n",
    "        combined_images = tf.concat([fake_image_and_labels, real_image_and_labels], axis=0)\n",
    "    \n",
    "        # Labels for discriminator\n",
    "        labels = tf.concat([tf.ones((batch_size, 1)), tf.zeros((batch_size, 1))], axis=0)\n",
    "    \n",
    "        # Train discriminator\n",
    "        with tf.GradientTape() as tape:\n",
    "            predictions = self.discriminator(combined_images)\n",
    "            d_loss = self.loss_fn(labels, predictions)\n",
    "        grads = tape.gradient(d_loss, self.discriminator.trainable_weights)\n",
    "        self.d_optimizer.apply_gradients(zip(grads, self.discriminator.trainable_weights))\n",
    "    \n",
    "        # Train generator\n",
    "        misleading_labels = tf.zeros((batch_size, 1))\n",
    "    \n",
    "        with tf.GradientTape() as tape:\n",
    "            fake_images = self.generator(random_vector_labels)\n",
    "            fake_image_and_labels = tf.concat([fake_images, image_one_hot_labels], axis=-1)\n",
    "            predictions = self.discriminator(fake_image_and_labels)\n",
    "            g_loss = self.loss_fn(misleading_labels, predictions)\n",
    "        \n",
    "            ph_losses = self.pd_loss(real_images, fake_images)\n",
    "            g_total_loss = g_loss + ph_losses\n",
    "            \n",
    "        grads = tape.gradient(g_total_loss, self.generator.trainable_weights)\n",
    "        self.g_optimizer.apply_gradients(zip(grads, self.generator.trainable_weights))\n",
    "    \n",
    "        # Update loss trackers\n",
    "        self.gen_loss_tracker.update_state(g_total_loss)\n",
    "        self.disc_loss_tracker.update_state(d_loss)\n",
    "        return {\n",
    "            \"loss\": self.gen_loss_tracker.result() + self.disc_loss_tracker.result(),\n",
    "            \"g_loss\": self.gen_loss_tracker.result(),\n",
    "            \"d_loss\": self.disc_loss_tracker.result(),\n",
    "        }"
   ],
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-23T22:54:54.498962Z",
     "start_time": "2025-02-23T22:54:54.263464Z"
    }
   },
   "cell_type": "code",
   "source": [
    "discriminator, generator = get_models()\n",
    "cond_gan = ConditionalGAN(discriminator=discriminator, generator=generator, latent_dim=latent_dim)\n",
    "cond_gan.compile(d_optimizer=opt, g_optimizer=opt, loss_fn=los)"
   ],
   "outputs": [],
   "execution_count": 15
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-SJbTfPtqECG",
    "ExecuteTime": {
     "end_time": "2024-11-22T23:10:10.564855800Z",
     "start_time": "2024-11-22T23:06:19.064800900Z"
    }
   },
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "source": "cond_gan.fit(train_dataset, epochs=num_epochs, batch_size=batch_size, verbose=1)",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-02-23T22:54:54.981303Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "   2/1875 [..............................] - ETA: 2:40:40 - loss: 2.3849 - g_loss: 1.7010 - d_loss: 0.6838"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
